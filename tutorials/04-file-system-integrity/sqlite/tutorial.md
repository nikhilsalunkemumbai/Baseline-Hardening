# SQLite Tutorial: File System Search and Integrity Analysis

## Introduction

While Bash, PowerShell, and Python are excellent for *performing* file system searches and calculating file hashes, SQLite provides an ideal solution for persistently *storing* and *querying* this collected metadata and integrity baselines. By centralizing file system audit data in an SQLite database, you can conduct historical trend analysis, detect unauthorized changes, and manage large sets of file integrity information with the full power of SQL. This tutorial will guide you through designing an SQLite schema for file system data and querying that data effectively.

## Framework Alignment

This tutorial on "**File System Search and Integrity Check**" using **SQLite** demonstrates the foundational skills necessary to implement components of a "Cross-Platform Baseline Hardening & Auditing Framework." The techniques learned here for storing and querying file system metadata and integrity baselines are directly applicable to auditing systems against defined security baselines and ensuring compliance.


## Why SQLite for File System Auditing?

*   **Historical Tracking:** Store multiple snapshots or baselines over time to track file additions, deletions, and modifications.
*   **Centralized Repository:** Consolidate file metadata and hashes from various directories or systems into a single, queryable database.
*   **Powerful SQL Analysis:** Use SQL to filter files by various criteria, identify specific hashes, compare baselines, and generate custom reports on file system integrity.
*   **Auditing & Compliance:** Easily query for files that deviate from a known good state or track access/modification patterns.
*   **Portable & Serverless:** The entire database is a single file, easily transferable and usable without a dedicated database server.
*   **Minimal Dependencies:** The `sqlite3` command-line tool and Python's `sqlite3` module are standard.

## Workflow: From Collection to SQLite

The typical workflow involves using a scripting language to:
1.  **Collect File Data:** Use Bash, PowerShell, or Python scripts (as demonstrated in previous tutorials) to traverse file systems, collect metadata, and compute hashes.
2.  **Structure Data:** Format the collected data (e.g., as a list of file metadata dictionaries, or a baseline mapping paths to hashes).
3.  **Ingest into SQLite:** Use a script (e.g., Python with `sqlite3` module) to read the structured data and insert it into an SQLite database.

For this tutorial, we will focus on the SQLite schema design and querying. We'll provide examples of how you might ingest data from a Python script that generates JSON reports or baselines.

## Schema Design for File System Auditing

A robust schema will typically involve several tables to represent snapshots/baselines and the files themselves, with appropriate links.

### Proposed Schema

1.  **`baselines` Table:** Stores metadata about each integrity baseline generated (when it was taken, what path it covers).
2.  **`files` Table:** Stores metadata for each file recorded in a baseline (path, size, modification time, hash). Each file entry will link to a specific `baseline_id`.

### 1. Creating the Database and Tables

We'll use a Python script to create the database and tables.

```python
import sqlite3
import json
import os
from datetime import datetime

DB_FILE = 'filesystem_audits.db'

def create_schema(conn):
    cursor = conn.cursor()

    # baselines table: stores metadata for each baseline run
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS baselines (
            baseline_id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT NOT NULL,
            root_path TEXT NOT NULL,
            algorithm TEXT NOT NULL
        )
    ''')

    # files table: stores details for each file within a baseline
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS files (
            file_id INTEGER PRIMARY KEY AUTOINCREMENT,
            baseline_id INTEGER NOT NULL,
            path TEXT NOT NULL,
            name TEXT NOT NULL,
            size INTEGER,
            last_modified TEXT,
            hash TEXT NOT NULL,
            FOREIGN KEY (baseline_id) REFERENCES baselines (baseline_id)
        )
    ''')
    
    # Add indexes for faster lookups
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_files_baseline_id ON files (baseline_id)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_files_path ON files (path)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_files_hash ON files (hash)')

    conn.commit()
    print("Database schema created/updated.")

# Example Usage:
# conn = sqlite3.connect(DB_FILE)
# create_schema(conn)
# conn.close()
```

### 2. Data Ingestion (Python Example)

Assuming you have JSON baseline files (`baseline_day1.json`, `baseline_day2.json`) generated by the Python `filesystem_auditor.py` script's `generate_baseline` command.

**`baseline_day1.json` (simplified):**
```json
{
  "path/to/file1.txt": {
    "hash": "hash1_day1",
    "size": 100,
    "mtime": "2026-02-20T10:00:00",
    "algorithm": "sha256"
  },
  "path/to/file2.txt": {
    "hash": "hash2_day1",
    "size": 200,
    "mtime": "2026-02-20T10:05:00",
    "algorithm": "sha256"
  }
}
```

**`baseline_day2.json` (modified file1, new file3):**
```json
{
  "path/to/file1.txt": {
    "hash": "hash1_day2_modified",
    "size": 105,
    "mtime": "2026-02-21T11:00:00",
    "algorithm": "sha256"
  },
  "path/to/file2.txt": {
    "hash": "hash2_day1",
    "size": 200,
    "mtime": "2026-02-20T10:05:00",
    "algorithm": "sha256"
  },
  "path/to/file3.txt": {
    "hash": "hash3_day2",
    "size": 50,
    "mtime": "2026-02-21T12:00:00",
    "algorithm": "sha256"
  }
}
```

```python
def ingest_baseline(conn, root_path, algorithm, json_data):
    cursor = conn.cursor()

    # Insert into baselines table
    cursor.execute('''
        INSERT INTO baselines (timestamp, root_path, algorithm)
        VALUES (?, ?, ?)
    ''', (datetime.now().isoformat(), root_path, algorithm))
    baseline_id = cursor.lastrowid

    # Iterate through file data
    for file_path_str, file_meta in json_data.items():
        # Extract filename from full path
        file_name = os.path.basename(file_path_str)
        cursor.execute('''
            INSERT INTO files (baseline_id, path, name, size, last_modified, hash)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (baseline_id, file_path_str, file_name, file_meta.get('size'),
              file_meta.get('mtime'), file_meta.get('hash')))

    conn.commit()
    print(f"Baseline for '{root_path}' (ID: {baseline_id}) ingested successfully.")

# Example Main function for ingestion:
# if __name__ == '__main__':
#     DB_FILE = 'filesystem_audits.db'
#     if os.path.exists(DB_FILE):
#         os.remove(DB_FILE)
#     conn = sqlite3.connect(DB_FILE)
#     create_schema(conn)

#     # Ingest baseline_day1.json
#     with open('baseline_day1.json', 'r') as f:
#         data1 = json.load(f)
#     ingest_baseline(conn, './test_data', 'sha256', data1)

#     # Ingest baseline_day2.json
#     with open('baseline_day2.json', 'r') as f:
#         data2 = json.load(f)
#     ingest_baseline(conn, './test_data', 'sha256', data2)
    
#     conn.close()
```

## Basic SQL Queries for File System Auditing

Once data is in SQLite, you can perform powerful analyses using the `sqlite3` command-line tool.

```bash
sqlite3 filesystem_audits.db
```
*(All following SQL commands are executed within the `sqlite3` prompt)*

### 1. List All Baselines

```sql
SELECT baseline_id, timestamp, root_path, algorithm FROM baselines;
```

### 2. Find All Files in the Latest Baseline for a Specific Root Path

```sql
-- First, find the latest baseline_id for a given root path
WITH LatestBaseline AS (
    SELECT MAX(baseline_id) as latest_id
    FROM baselines
    WHERE root_path = './test_data' -- Adjust as needed
)
SELECT f.path, f.hash, f.size, f.last_modified
FROM files f
JOIN LatestBaseline lb ON f.baseline_id = lb.latest_id;
```

### 3. Find Files Larger Than a Specific Size in a Baseline

```sql
SELECT b.root_path, f.path, f.size
FROM baselines b
JOIN files f ON b.baseline_id = f.baseline_id
WHERE b.baseline_id = 1 AND f.size > 150; -- Replace 1 with the target baseline_id
```

### 4. Identify Files with a Specific Hash

```sql
SELECT b.timestamp, b.root_path, f.path, f.hash
FROM baselines b
JOIN files f ON b.baseline_id = f.baseline_id
WHERE f.hash = 'hash1_day1'; -- Replace with actual hash
```

### 5. Detect Modified Files Between Two Baselines

This requires a `LEFT JOIN` and comparison. Assume `baseline_id = 1` is Day 1 and `baseline_id = 2` is Day 2.

```sql
SELECT
    f_day1.path,
    f_day1.hash AS hash_day1,
    f_day2.hash AS hash_day2
FROM
    files f_day1
LEFT JOIN
    files f_day2 ON f_day1.path = f_day2.path AND f_day2.baseline_id = 2 -- Baseline for Day 2
WHERE
    f_day1.baseline_id = 1 -- Baseline for Day 1
    AND (f_day2.hash IS NULL OR f_day1.hash <> f_day2.hash);
    -- f_day2.hash IS NULL means file was deleted or not in Day 2 baseline
    -- f_day1.hash <> f_day2.hash means file was modified
```

### 6. Detect New Files Added in a Later Baseline

```sql
SELECT
    f_day2.path,
    f_day2.hash AS hash_day2
FROM
    files f_day2
LEFT JOIN
    files f_day1 ON f_day2.path = f_day1.path AND f_day1.baseline_id = 1 -- Baseline for Day 1
WHERE
    f_day2.baseline_id = 2 -- Baseline for Day 2
    AND f_day1.file_id IS NULL; -- File exists in Day 2, but not in Day 1
```

### 7. Detect Deleted Files Between Two Baselines

```sql
SELECT
    f_day1.path,
    f_day1.hash AS hash_day1
FROM
    files f_day1
LEFT JOIN
    files f_day2 ON f_day1.path = f_day2.path AND f_day2.baseline_id = 2 -- Baseline for Day 2
WHERE
    f_day1.baseline_id = 1 -- Baseline for Day 1
    AND f_day2.file_id IS NULL; -- File exists in Day 1, but not in Day 2
```

## Guiding Principles with SQLite

*   **Portability:** The SQLite database file (`.db`) is fully portable. The SQL queries are standard.
*   **Efficiency:** SQLite is highly optimized for read operations, especially with proper indexing. Storing structured data allows for very efficient querying for integrity checks.
*   **Minimal Dependencies:** Python's built-in `sqlite3` module and the `sqlite3` CLI tool are standard components.
*   **CLI-centric:** The `sqlite3` command-line tool provides a robust interface for interacting with the database.
*   **Structured Data & Actionable Output:** Storing file system metadata and hashes in a relational database transforms raw audit data into structured, queryable information, enabling powerful analysis, reporting, and forensic investigations.

## Conclusion

SQLite provides a robust and indispensable backbone for managing file system search results and integrity baselines. It transforms transient file system data into persistent, queryable knowledge, enabling in-depth auditing, historical tracking of changes, and the rapid detection of unauthorized modifications. By integrating SQLite with your chosen scripting language for data collection and ingestion, you create a comprehensive and flexible file system monitoring and integrity framework. The next step is to apply this knowledge in practical exercises.